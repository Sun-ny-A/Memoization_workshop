

On 9/22/23, I went to the student activities workshop on memoization. Memoization is a way to improve performance with complex functions that otherwise would not be as efficient to run. It uses a caching system that returns the cached code when the complex function is called. A cache is a way to store data and information. In this way, the stored code increases efficiency which is especially useful with repetitive calculations. Memoization is also useful when working with outside APIs. Other companies may have a rate limit on their APIs so memoization can help stay within those limits. I also learned how to pronounce cache, which I'm very happy about.

There are 3 different approaches to using memoizaton. The first is to use a dictionary to store a customer cached function. This provides fast access as it stores cache in memory. The second way is to use functool.lru_cache which is a python library tat can be imported. It uses a Least Recently Used (LRU) cache. It continues to store code that is continuously being used but gets rid of code that hasn't been retrieved for a while. For example, one may only be interested in the last 5 values stored. By setting a maxsize of 5 values it will disregard everything beyond that. The third way is to joblib. This approach stores cache on the harddrive (instead of the memory).

There are some limitations to these approaches. Cache storage can be an issue if this is taking up most of the memory on the computer. It may not be the best way to store large code for an extended period of time. If the data being used is not fixed and can be changed over time that will also lead to issues in retrieving stored cache as it may no longer be useful. It could be considered stale data or stale cache. For large data joblib is better but it will run slower as it's on a harddrive instead of the memory.

